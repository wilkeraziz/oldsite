\section*{Teaching}

%\begin{description}

I am highly committed to education and I believe in empowering students with knowledge in a healthy manner. %thrive to inspire a positive and healthy energy in class. %aiming at knowledge transfer without trauma. 

Since 2015, I coordinate, design, and implement MSc and BSc courses offered at UvA covering topics such as:
\begin{itemize}
	\setlength\itemsep{1pt}
	\item probabilistic graphical models and Bayesian methods for NLP
	\item approximate probabilistic inference: Markov chain Monte Carlo sampling and variational inference
	\item weighted automata and grammars, semirings, and deductive systems
	\item statistical and neural approaches to natural language processing
\end{itemize}
I mostly design my own materials and value depth and quality. 


\begin{center}
\begin{tabular}{l p{0.8\textwidth}} 
    \toprule
{\bf Course} & {\it Deep Learning for Natural Language Processing} \\
{\bf Role}   & Lecturer (2019--present) \\
 & ~offered in collaboration with Christof Monz (IvI)\\
{\bf Programme} & Master's of AI (UvA)\\
{\bf Description} & The course covers advanced supervised and unsupervised learning techniques in natural language processing with a focus on statistical learning powered by deep neural networks.
\\ \midrule
{\bf Course} & {\it Basic Probability Theory} \\
{\bf Role}   & Coordinator (2019--present) \\
{\bf Programme} & Master's of Logic (UvA)\\
{\bf URL} & \url{https://basicprobability.github.io} \\
%{\bf Syllabus} & \url{https://uva-slpl.github.io/ull/syllabus.html}\\
{\bf Description} & The course covers the basics of combinatorics, axiomatic probability theory, discrete and continuous random variables, and maximum likelihood estimation. 
\\ \midrule
{\bf Course} & {\it Natural Language Models and Interfaces} \\
{\bf Role}   & Coordinator (2018--present) \\
{\bf Programme} & Bachelor's of AI (UvA)\\
{\bf URL} & \url{https://uva-slpl.github.io/nlmi/} \\
%{\bf Syllabus} & \url{https://uva-slpl.github.io/nlmi/lectures.html} \\
{\bf Description} & The course covers some of the essential techniques in natural language processing with a focus on language modelling and word representation.
\\ \midrule
{\bf Course} & {\it Unsupervised Language Learning} \\
{\bf Role}   & Lecturer (2018) \\
 & ~offered in collaboration with Ekaterina Shutova (ILLC)\\
{\bf Programme} & Master's of AI (UvA)\\
{\bf URL} & \url{https://uva-slpl.github.io/ull/} \\
%{\bf Syllabus} & \url{https://uva-slpl.github.io/ull/syllabus.html}\\
{\bf Description} & The course covers advanced unsupervised learning techniques in natural language processing with a focus on meaning representation (including deep generative models of word and sentence representation).
\\ \midrule
{\bf Course} & {\it Natural Language Processing 2} \\
{\bf Role}   & Lecturer (2015-2017) \\
{\bf Programme} & Master's of AI (UvA)\\
{\bf URL} & \url{https://uva-slpl.github.io/nlp2/} \\
%{\bf Syllabus} & \url{https://uva-slpl.github.io/nlp2/2017} \\
{\bf Description} & The course covers structure prediction problems related to translation (e.g. unsupervised alignment, synchronous grammar induction, statistical and neural MT). 
\\ 
\bottomrule
\end{tabular}
\end{center}

~

For a complete list of courses and projects including available material and project outcomes, please refer to \url{https://wilkeraziz.github.io/teaching.html}.
For invited lectures and talks, please refer to \url{https://wilkeraziz.github.io/talks.html}.

~


I have recently developed a tutorial on variational inference and deep generative models for NLP audiences. This is a joint effort with Philip Schulz, and we have been taking this tutorial (or parts of it) to diverse audiences at universities (e.g. Melbourne, Monash, Macquarie, Amsterdam, Heidelberg, and Instituto Superior T\'{e}cnico in Lisbon), companies (e.g. Amazon, Naver Labs, Yandex), and international conferences (ACL 2018). For more information check our schedule and available resources: \url{https://vitutorial.github.io}.

%\\

%Experience in research and development of SMT algorithms, e.g. grammar extraction (via pattern matching) and decoding (optimisation and sampling), MT evaluation as well as other NLP applications such as sentence- and word-alignment, word-sense disambiguation, lexical substitution and paraphrasing through pivoting. 

%Experience in exploiting machine learning techniques to model natural language tasks such as named-entity recognition and semantic role labelling.

