\section*{EDUCATION}

\begin{tabular}{p{2cm}  p{13.92cm}}
2010--2014 & \bf Ph.D. Computational Linguistics  \\
 & Research Institute in Information and Language Processing â€“ University of Wolverhampton \\
 & {\bf Thesis:} Exact Sampling and Optimisation in Statistical Machine Translation \\
 & {\bf Supervisors:} \pbox[t]{13.92cm}{Dr. Lucia Specia (University of Sheffield) \\ Dr. Marc Dymetman (Xerox Research Centre in Europe) \\ Prof. Dr. Ruslan Mitkov (University of Wolverhampton)} \\
 & {\bf Summary:} In statistical machine translation, inference is performed over a high-complexity discrete distribution defined by the intersection between a translation hypergraph and a target language model. This distribution is too complex to be represented exactly and one typically resorts to approximation techniques either to perform optimisation -- the task of searching for the optimum translation derivation -- or sampling -- the task of finding a subset of translation derivations that is statistically representative of the goal distribution.  This thesis introduces an approach to exact optimisation and sampling based on a form of adaptive rejection sampling. In this view, the intractable goal distribution is upperbounded by a simpler, thus tractable, proxy distribution which is then incrementally refined to be closer to the goal until the maximum is found, or until the sampling performance exceeds a certain level.\\
 & \\
2005--2010 	& \bf B.Sc. Computer Engineering  \\ %(overall mark 83/100)
 & Escola de Engenharia de S\~ao Carlos - Universidade Estadual de S\~ao Paulo (USP) \\
 & {\bf Monograph:} Lexical Substitution for Statistical Machine Translation \\
 & {\bf Summary:} I proposed a context model based on word co-occurrence to perform cross-language lexical substitution. I used passive-aggressive supervised learning to fit a linear model to rank translation alternatives in context. The results of this work were reported in the Cross-Language Lexical Substitution Task at SemEval-2010. \\
 & {\bf Research experience:} I spent one year (from March 2009 to February 2010) at the Xerox Research Centre in Europe (Grenoble, France) where I worked on the use of context models and textual entailment to handle out-of-vocabulary words in SMT. My project was supervised by Dr. Marc Dymetman and funded by the Pascal-2 European Network of Excellence.\\
\end{tabular}

%language lexical substitution. First a model is learnt from parallel data by computing the mutual information between: i) source words in a sentence (gives a notion of how important each word is to describe the contexts of a given word) and ii) target words and the source words happening in the input context (reflects the most likely alignments). Finally at test time the translation candidates are scored in context following a weighted-average fashion: the fitness in context is given by averaging the strength of association between the target candidate and the source words in the context, weighted by how relevant those source words are at describing the original polysemous word. Besides the context model described, a few other features, such as the standard word translation probability (adds a notion of most frequent sense), were used.

% and our system scored 2$^{nd}$ best in finding the best target replacement and 4$^{th}$ best in finding the 10-best target replacements out of 14 participating systems.

