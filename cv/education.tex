\section*{Education}

\begin{tabular}{p{2cm}  p{13.92cm}}
2010--2014 & \bf Ph.D. Computational Linguistics  \\
 & Research Institute in Information and Language Processing â€“ University of Wolverhampton \\
 & {\bf Thesis:} Exact Sampling and Optimisation in Statistical Machine Translation \\
 & {\bf Supervisors:} Prof. Dr. Lucia Specia, Dr. Marc Dymetman, Prof. Dr. Ruslan Mitkov \\
 & {\bf Summary:} I introduce an approach to exact optimisation and sampling based on a form of adaptive rejection sampling which addresses challenges in global optimisation and unbiased sampling in high-dimensional discrete spaces. In this view, an intractable goal distribution is upperbounded by a tractable proxy distribution which is then incrementally refined to be closer to the goal. \\ % until the maximum is found, or until sampling performance exceeds a certain level.\\
 & \\
2005--2010 	& \bf B.Sc. Computer Engineering  \\ %(overall mark 83/100)
 & Escola de Engenharia de S\~ao Carlos - Universidade Estadual de S\~ao Paulo (USP) \\
 & {\bf Monograph:} Lexical Substitution for Statistical Machine Translation \\
 & {\bf Summary:} I propose a context model based on word co-occurrence and supervised learning to rank for cross-language lexical substitution. %My findings were reported to the Cross-Language Lexical Substitution Task at SemEval-2010. \\
% & {\bf Research experience:} I spent one year (from March 2009 to February 2010) at the Xerox Research Centre in Europe (Grenoble, France) where I worked on the use of context models and textual entailment to handle out-of-vocabulary words in SMT. My project was supervised by Dr. Marc Dymetman and funded by the Pascal-2 European Network of Excellence.\\
\end{tabular}

%language lexical substitution. First a model is learnt from parallel data by computing the mutual information between: i) source words in a sentence (gives a notion of how important each word is to describe the contexts of a given word) and ii) target words and the source words happening in the input context (reflects the most likely alignments). Finally at test time the translation candidates are scored in context following a weighted-average fashion: the fitness in context is given by averaging the strength of association between the target candidate and the source words in the context, weighted by how relevant those source words are at describing the original polysemous word. Besides the context model described, a few other features, such as the standard word translation probability (adds a notion of most frequent sense), were used.

% and our system scored 2$^{nd}$ best in finding the best target replacement and 4$^{th}$ best in finding the 10-best target replacements out of 14 participating systems.

