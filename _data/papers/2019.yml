-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  img: latentrationale
  title: "Interpretable Neural Predictions with Differentiable Binary Variables"
  authors: "Jasmijn Bastings, Wilker Aziz and Ivan Titov"
  doc-url: //arxiv.org/pdf/1905.08160.pdf
  arxiv: //arxiv.org/abs/1905.08160
  slides:
  booktitle: "ACL"  
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  img: visualclues
  title: "Latent Variable Model for Multi-modal Translation"
  authors: "Iacer Calixto, Miguel Rios and Wilker Aziz"
  doc-url: //www.aclweb.org/anthology/P19-1642.pdf
  arxiv: //arxiv.org/abs/1811.00357
  slides:
  booktitle: "ACL"
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  img: bnaf
  title: "Block Neural Autoregressive Flow"
  authors: "Nicola De Cao, Wilker Aziz and Ivan Titov"
  doc-url: https://arxiv.org/pdf/1904.04676.pdf
  arxiv: https://arxiv.org/abs/1904.04676
  booktitle: "UAI"
  slides:
  abstract: >
      Normalising flows (NFS) map two density functions via a differentiable bijection whose Jacobian determinant can be computed efficiently. Recently, as an alternative to hand-crafted bijections, Huang et al. (2018) proposed neural autoregressive flow (NAF) which is a universal approximator for density functions. Their flow is a neural network (NN) whose parameters are predicted by another NN. The latter grows quadratically with the size of the former and thus an efficient technique for parametrization is needed. We propose block neural autoregressive flow (B-NAF), a much more compact universal approximator of density functions, where we model a bijection directly using a single feed-forward network. Invertibility is ensured by carefully designing each affine transformation with block matrices that make the flow autoregressive and (strictly) monotone. We compare B-NAF to NAF and other established flows on density estimation and approximate inference for latent variable models. Our proposed flow is competitive across datasets while using orders of magnitude fewer parameters.
-
  layout: paper
  paper-type: inproceedings
  selected: y
  year: 2019
  img: entitygcn
  title: "Question Answering by Reasoning Across Documents with Graph Convolutional Networks"
  authors: "Nicola De Cao, Wilker Aziz and Ivan Titov"
  doc-url: //arxiv.org/pdf/1808.09920.pdf
  arxiv: //arxiv.org/abs/1808.09920
  booktitle: "NAACL"
  slides:
  abstract: >
    Most research in reading comprehension has focused on answering questions based on individual documents or even single paragraphs. We introduce a neural model which integrates and reasons relying on information spread within documents and across multiple documents. We frame it as an inference problem on a graph. Mentions of entities are nodes of this graph while edges encode relations between different mentions (e.g., within- and cross-document coreference). Graph convolutional networks (GCNs) are applied to these graphs and trained to perform multi-step reasoning. Our Entity-GCN method is scalable and compact, and it achieves state-of-the-art results on a multi-document question answering dataset, WIKIHOP (Welbl et al., 2018).      
